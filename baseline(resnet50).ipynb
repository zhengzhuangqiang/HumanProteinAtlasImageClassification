{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:48.523014Z",
     "start_time": "2018-11-28T14:00:48.518908Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.insert(0, '/sdadata/zzq/work/HumanProteinAtlasImageClassification')\n",
    "\n",
    "IMAGE_CHANNELS = 4\n",
    "IMAGE_SIZE = [512, 512]\n",
    "MODEL_NAME = 'resnet50'\n",
    "IMAGES_DIR = '/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/images_grby_landmarks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logger class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import handlers\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self,filename,level='info',\\\n",
    "                 when='D',backCount=3,\\\n",
    "                 fmt='%(asctime)s\\n%(message)s'):\n",
    "        self.level_relations = {\n",
    "            'debug':logging.DEBUG,\n",
    "            'info':logging.INFO,\n",
    "            'warning':logging.WARNING,\n",
    "            'error':logging.ERROR,\n",
    "            'crit':logging.CRITICAL\n",
    "        }\n",
    "        self.logger = logging.getLogger(filename)\n",
    "        format_str = logging.Formatter(fmt)#设置日志格式\n",
    "        self.logger.setLevel(self.level_relations.get(level))#设置日志级别\n",
    "        sh = logging.StreamHandler()#往屏幕上输出\n",
    "        sh.setFormatter(format_str) #设置屏幕上显示的格式\n",
    "        th = handlers.TimedRotatingFileHandler(filename=filename,when=when,backupCount=backCount,\\\n",
    "                                               encoding='utf-8')#往文件里写入#指定间隔时间自动生成文件的处理器\n",
    "        th.setFormatter(format_str)#设置文件里写入的格式\n",
    "        self.logger.addHandler(sh) #把对象加到logger里\n",
    "        self.logger.addHandler(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:51.819658Z",
     "start_time": "2018-11-28T14:00:48.998700Z"
    },
    "code_folding": [
     0,
     12,
     20,
     22,
     26,
     37,
     41,
     46,
     54,
     58,
     74,
     90,
     95
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# import torch\n",
    "# from imgaug import augmenters as iaa\n",
    "# from common.slim.preprocessing.inception_preprocessing import preprocess_image, preprocess_for_train, preprocess_for_eval\n",
    "\n",
    "# class MyDataset(object):\n",
    "#     \"\"\"\n",
    "#         read_one_sample-->read_one_batch-->preprocess-->getitem\n",
    "#     \"\"\"\n",
    "#     def __init__(self, df, **kw):\n",
    "#         self.df, self.kw = df, kw\n",
    "#         self.str_df = self.df2str(df)\n",
    "#         self.graph = tf.Graph()\n",
    "#         with self.graph.as_default():\n",
    "#             self.__one_sample = self.read_one_sample(self.str_df, kw)\n",
    "#             self.__batch_samples = self.read_batch_samples(self.__one_sample, kw)\n",
    "#             self.open_session()\n",
    "#     def __len__(self,):\n",
    "#         return int(np.ceil(self.df.shape[0]/self.kw['batch_size']))\n",
    "#     def __getitem__(self, index):\n",
    "#         res = self.sess.run(self.__batch_samples) #  读取原始样本\n",
    "#         res[1] = self.preprocess(res[1]) # res[1]为图片 对图像进行预处理\n",
    "#         return res\n",
    "#     def open_session(self, gpu_memory_fraction=None):\n",
    "#         if gpu_memory_fraction is None:\n",
    "#             config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "#             config.gpu_options.allow_growth = True\n",
    "#         else:\n",
    "#             gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "#             config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True, allow_soft_placement=True)\n",
    "#         self.sess = tf.Session(graph=self.graph, config=config)\n",
    "#         self.sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "#         self.coord = tf.train.Coordinator()\n",
    "#         self.threads = tf.train.start_queue_runners(sess=self.sess, coord=self.coord)\n",
    "#     def close_session(self):\n",
    "#         self.coord.request_stop() \n",
    "#         self.coord.join(self.threads)\n",
    "#         self.sess.close()\n",
    "#     def df2str(self, df):\n",
    "#         def other2str(x):\n",
    "#             return [str(val) for val in x]\n",
    "#         data = [','.join(other2str(df.iloc[i])) for i in range(df.shape[0])]\n",
    "#         return data\n",
    "#     def read_one_sample(self, data, kw):\n",
    "#         \"\"\"\n",
    "#             读取原始样本（未经过预处理）\n",
    "#         \"\"\"\n",
    "#         data = tf.cast(data, tf.string)\n",
    "#         value = tf.train.slice_input_producer(tensor_list=[data], shuffle=kw['shuffle'],\\\n",
    "#                                               num_epochs=kw['num_epochs'], capacity=self.df.shape[0])[0]\n",
    "#         #训练集、测试集图片所在目录不同\n",
    "#         if kw['is_test'] is True:\n",
    "#             record = tf.decode_csv(value, record_defaults=[['img_name']], field_delim=',') #这里读取的为一行元素\n",
    "#             img_name = tf.cast(record[0], dtype=tf.string)\n",
    "#             img_path = tf.string_join([kw['img_dir'], 'test', img_name], separator='/')\n",
    "#         else:\n",
    "#             record_defaults =[[0] for i in range(kw['num_labels'])]\n",
    "#             record_defaults.insert(0, ['img_name'])\n",
    "#             record = tf.decode_csv(value, record_defaults=record_defaults, field_delim=',') #这里读取的为一行元素\n",
    "#             img_name = tf.cast(record[0], dtype=tf.string)\n",
    "#             img_path = tf.string_join([kw['img_dir'], 'train', img_name], separator='/')\n",
    "#             label = tf.stack(record[1:], axis=-1)\n",
    "            \n",
    "#         img = self.read_img(img_path, kw)\n",
    "        \n",
    "#         # 训练集测试集返回值不同\n",
    "#         if kw['is_test'] is True:\n",
    "#             return img_name, img\n",
    "#         else:\n",
    "#             label.set_shape(kw['num_labels'])\n",
    "#             return  img_name, img, label\n",
    "#     def read_img(self, img_path, kw):\n",
    "#         img_red_path = img_path + '_red.png'\n",
    "#         img_green_path = img_path +'_green.png'\n",
    "#         img_blue_path = img_path + '_blue.png'\n",
    "#         img_yellow_path = img_path + '_yellow.png'\n",
    "        \n",
    "#         imgR = self.__read_img(img_red_path, kw)\n",
    "#         imgG = self.__read_img(img_green_path, kw)\n",
    "#         imgB = self.__read_img(img_blue_path, kw)\n",
    "#         if IMAGE_CHANNELS == 3:\n",
    "#             img = tf.concat([imgR, imgG, imgB], axis=-1)\n",
    "#         else: #为4通道\n",
    "#             imgY = self.__read_img(img_yellow_path, kw)\n",
    "#             img = tf.concat([imgR, imgG, imgB, imgY], axis=-1)\n",
    "#         img.set_shape([512, 512, IMAGE_CHANNELS])\n",
    "#         return img\n",
    "#     def __read_img(self, img_path, kw):\n",
    "#         img = tf.read_file(img_path)\n",
    "#         img = tf.image.decode_png(img, channels=1)  \n",
    "#         img.set_shape([512, 512, 1])\n",
    "#         return img    \n",
    "#     def preprocess(self, imgs):\n",
    "        \n",
    "#         if self.kw['is_test'] is False:\n",
    "#             if self.kw['img_aug'] is True:\n",
    "#                 imgs = self.img_augmention(imgs)\n",
    "        \n",
    "#         #标准化\n",
    "#         imgs = imgs/255.0\n",
    "#         mean, std = [0.08362823, 0.05534997, 0.05827458, 0.08680427], [0.14305845, 0.10747696, 0.15899317, 0.14487827]\n",
    "#         imgs = (imgs - mean[0:IMAGE_CHANNELS])/std[0:IMAGE_CHANNELS] \n",
    "#         imgs = np.transpose(imgs, (0, 3, 1, 2)) #channels first\n",
    "#         return imgs\n",
    "#     def img_augmention(self, img):\n",
    "#         seq = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Affine(shear=(-16, 16)),\n",
    "#                 iaa.ElasticTransformation(sigma=(0,3)),\n",
    "#                 iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "                \n",
    "#             ])], random_order=True)\n",
    "#         img = seq.augment_images(img)\n",
    "#         return img\n",
    "#     def read_batch_samples(self, one_sample, kw):\n",
    "#         capacity = 1000 + kw['batch_size'] * 3\n",
    "#         batch_samples = tf.train.batch(tensors=one_sample, \\\n",
    "#                                        batch_size=kw['batch_size'], \\\n",
    "#                                        num_threads=kw['num_threads'], \\\n",
    "#                                        capacity=capacity, allow_smaller_final_batch=True)\n",
    "#         return batch_samples\n",
    "#     def visualize(self, ):\n",
    "#         record = self.__getitem__(0)\n",
    "#         imgs = record[1]\n",
    "#         n_rows, n_cols = 4, 4\n",
    "#         plt.figure(figsize=(n_rows*4, n_cols*4))\n",
    "#         for i in range(n_rows):\n",
    "#             for j in range(n_cols):\n",
    "#                 plt.subplot(n_rows, n_cols, i*n_cols+j+1)\n",
    "#                 plt.imshow(np.transpose(imgs[i*n_cols+j], [1,2,0])[:,:,:3])\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     28,
     30,
     32,
     46,
     59,
     77,
     79,
     83,
     93,
     95,
     97
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, img_aug=False, is_test=False):\n",
    "        self.df, self.img_aug, self.is_test = df, img_aug, is_test\n",
    "        self.img_dir = os.path.join(IMAGES_DIR, 'test' if is_test else 'train')\n",
    "        self.img_suffixs = ['_green.png', '_red.png', '_blue.png', '_yellow.png']\n",
    "        # for GRBY\n",
    "        #self.mean = np.array([0.04531973, 0.05907663, 0.04067749, 0.05924788])\n",
    "        #self.std = np.array([0.1048129 , 0.12819295, 0.1317363 , 0.12461057])\n",
    "        # for GRBY_landmarks\n",
    "        self.mean = np.array([0.06306252, 0.10562667, 0.07949321, 0.10858839])\n",
    "        self.std = np.array([0.80647908, 0.82780679, 0.82680373, 0.8309615 ])\n",
    "    def __len__(self,):\n",
    "        return int(self.df.shape[0])\n",
    "    def __getitem__(self, index):\n",
    "        record = self.df.iloc[index].values\n",
    "        img = self.read_img(record[1], mode=1)\n",
    "        if self.img_aug is True:\n",
    "            img = self.img_aug_func(img)\n",
    "        #数据标准化\n",
    "        img = img/255.0\n",
    "        img = (img-self.mean)/self.std\n",
    "        img = np.transpose(img, (2,0,1))\n",
    "        img = img[-IMAGE_CHANNELS:]\n",
    "        if self.is_test is True:\n",
    "            return record[0], img.astype(np.float32)\n",
    "        else:\n",
    "            return record[0], img.astype(np.float32), np.array(record[2:], dtype=np.int32)\n",
    "    def read_img(self, img_name, mode=0):\n",
    "        \"\"\"\n",
    "            根据图片名称加载四张单色图像\n",
    "            return: [G, R, B, Y], shape=[512, 512, 4]\n",
    "        \"\"\"\n",
    "        img_path = '/'.join([self.img_dir, img_name])\n",
    "        if mode==0:\n",
    "            img_path = [img_path+val for val in self.img_suffixs]\n",
    "            img = [io.imread(val) for val in img_path]\n",
    "            img = np.stack(img, axis=-1)\n",
    "        else:\n",
    "            img_path = img_path+'.png'\n",
    "            img = io.imread(img_path)\n",
    "        return img\n",
    "    def mark_img(self, img):\n",
    "        \"\"\"\n",
    "            根据R,B,Y三个通道分别标记G通道\n",
    "            return: G通道加三个被标记的通道，共计四通道\n",
    "        \"\"\"\n",
    "        G, R, B, Y = np.split(img, axis=-1, indices_or_sections=img.shape[-1])# 此时RGBY的shape均为512x512x1\n",
    "        R, B, Y = R>np.mean(R), B>np.mean(B), Y>np.mean(Y)\n",
    "        G = np.repeat(G, repeats=3, axis=-1)\n",
    "        R = ia.SegmentationMapOnImage(R, shape=G.shape).draw_on_image(G)[...,0]\n",
    "        B = ia.SegmentationMapOnImage(B, shape=G.shape).draw_on_image(G)[...,0]\n",
    "        Y = ia.SegmentationMapOnImage(Y, shape=G.shape).draw_on_image(G)[...,0]\n",
    "        img = np.stack([G[...,0], R, B, Y], axis=-1)\n",
    "        return img\n",
    "    def img_aug_func(self, img):\n",
    "        \"\"\"\n",
    "            step1: 填充一律用0，因为从对原始图像的可视化可以指导，背景是黑色的\n",
    "            step2: 注意加噪音，因为部分小的细胞器本来就是一些琐碎的点，这样可以增强模型的鲁棒性\n",
    "            #注意：seq输入图片的shape为： [N, H, W, C]\n",
    "        \"\"\"\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(shear=(-16, 16)),#扭曲\n",
    "                iaa.ElasticTransformation(sigma=(0,3)),#局部变化\n",
    "                iaa.PiecewiseAffine(scale=(0.01, 0.05), nb_rows=8, nb_cols=8), # 局部扭曲\n",
    "                iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),#加高斯噪音\n",
    "            ])], random_order=True)\n",
    "        img = seq.augment_images(img)[0]\n",
    "        return img\n",
    "class MyLoader(object):\n",
    "    def __init__(self, df, img_aug=False, is_test=False, batch_size=16, shuffle=False, num_workers=1):\n",
    "        self.dataset = MyDataset(df, img_aug, is_test)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size, shuffle, \\\n",
    "                                     num_workers=num_workers, pin_memory=True)\n",
    "        self.iter = self.__iter__()\n",
    "        self.batch_size = batch_size\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            为了兼容之前用tensorflow的书写规范\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tmp = next(self.iter)\n",
    "        except StopIteration as e:\n",
    "            self.iter =self.__iter__()\n",
    "            return self.__getitem__(0)\n",
    "        return tmp\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataset)/self.batch_size))\n",
    "    def visualize(self):\n",
    "        record = self.__getitem__(0)\n",
    "        img_names = record[0]\n",
    "        imgs = record[1]\n",
    "        n_rows, n_cols = 4, 4\n",
    "        plt.figure(figsize=(n_rows*4, n_cols*4))\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                plt.subplot(n_rows, n_cols, i*n_cols+j+1)\n",
    "                plt.imshow(np.transpose(imgs[i*n_cols+j], [1,2,0])[:,:,-3:])\n",
    "                plt.title(s=str(img_names[i*n_cols+j]))\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:53.329079Z",
     "start_time": "2018-11-28T14:00:51.821405Z"
    },
    "code_folding": [
     20
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import importlib\n",
    "import common.models_pytorch\n",
    "importlib.reload(common.models_pytorch.resnet)\n",
    "from common.models_pytorch import resnet\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "class MyPredictor(nn.Module):\n",
    "    def __init__(self, num_output):\n",
    "        super(MyPredictor, self).__init__()\n",
    "        if MODEL_NAME is 'resnet50':\n",
    "            self.features = resnet.resnet50(pretrained=True, in_channels=IMAGE_CHANNELS)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=(1,1)) \n",
    "        self.classifier = nn.Linear(self.features.out_features, num_output)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)  \n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:53.448681Z",
     "start_time": "2018-11-28T14:00:53.331685Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyBCELoss, self).__init__()\n",
    "        self.loss = nn.BCEWithLogitsLoss().to(device)\n",
    "    def forward(self, target, input):\n",
    "        loss = self.loss(target=target, input=input)\n",
    "        return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "class MyFocalLossWithWeight(nn.Module):\n",
    "    def __init__(self, alpha, device, gamma=2):\n",
    "        super(MyFocalLossWithWeight, self).__init__()\n",
    "        self.alpha = torch.Tensor(alpha).float().to(device)\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "    def forward(self, input, target):\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        logp_ = input + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        logp = logp_ - input\n",
    "        loss = self.alpha*target*logp + (1-self.alpha)*(1-target)*logp_\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:53.535650Z",
     "start_time": "2018-11-28T14:00:53.451429Z"
    },
    "code_folding": [
     41,
     76
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "class WorkFlow(object):\n",
    "    def __init__(self, classifier=None, loss=None, optim=None, **params):\n",
    "        self.classifier = classifier\n",
    "        self.loss = loss\n",
    "        self.optim = optim\n",
    "        self.init_params = params\n",
    "        self.save_path = params['save_path']\n",
    "    def lr_find(self, train_dataset, n_points=200, n_repeat=4):\n",
    "        scheduler = LambdaLR(self.optim, lr_lambda=lambda epoch: 1.1**epoch)\n",
    "        res = {'lr':[], 'loss':[]}\n",
    "        self.classifier.train()\n",
    "        i = 0\n",
    "        min_loss, max_loss = 100000000, -100000000\n",
    "        while i<n_points:\n",
    "            tmp_ = []\n",
    "            for j in range(n_repeat):\n",
    "                sample_ids, images, labels = train_dataset[0]\n",
    "                images = torch.FloatTensor(images).to(self.init_params['device'])\n",
    "                labels = torch.FloatTensor(labels).to(self.init_params['device'])\n",
    "                outputs = self.classifier(images)\n",
    "                loss = self.loss(target=labels, input=outputs)\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                tmp_.append(loss.item())\n",
    "            cur_loss = np.mean(tmp_)\n",
    "            min_loss = min(min_loss, cur_loss)\n",
    "            max_loss = max(max_loss, cur_loss)\n",
    "            res['loss'].append(cur_loss)\n",
    "            res['lr'].append(scheduler.get_lr())\n",
    "            if (i>50) and (cur_loss>min_loss) and (cur_loss>((min_loss+max_loss)/2)) :\n",
    "                break\n",
    "            i += 1\n",
    "            print(i, end='\\r')\n",
    "            scheduler.step()\n",
    "        return res\n",
    "    def set_lr_weightdecay(self, lr=None, lr_group=None, weight_decay=None, weight_decay_group=None):\n",
    "        if lr is not None:\n",
    "            for i, params in enumerate(self.optim.param_groups):\n",
    "                params['lr'] = lr\n",
    "        if lr_group is not None:\n",
    "            for i, params in enumerate(self.optim.param_groups):\n",
    "                params['lr'] = lr_group[i]\n",
    "        if weight_decay is not None:\n",
    "            for i, params in enumerate(self.optim.param_groups):\n",
    "                params['weight_decay'] = weight_decay\n",
    "        if weight_decay_group is not None:\n",
    "            for i, params in enumerate(self.optim.param_groups):\n",
    "                params['weight_decay'] = weight_decay_group[i]\n",
    "    def train(self, train_dataset, params, val_dataset=None):\n",
    "        params['train_steps'] = len(train_dataset) if params['train_steps'] is None else params['train_steps']\n",
    "        if val_dataset is not None:\n",
    "            params['valid_steps'] = len(valid_dataset) if params['valid_steps'] is None else params['valid_steps']\n",
    "        self.train_params = params\n",
    "        res, s = self.evaluation(val_dataset, params['valid_steps'], pivot=params['pivot'])\n",
    "        print(\"start-->\", s)\n",
    "        best_loss = res['loss']\n",
    "        best_f1 = res['f1']\n",
    "        if 'history' not in dir(self):\n",
    "            self.history = {\n",
    "                'train_loss': [], 'train_pre': [], 'train_rec': [], 'train_f1': [],\n",
    "                'valid_loss': [], 'valid_pre': [], 'valid_rec': [], 'valid_f1': [],\n",
    "            }\n",
    "            self.start_step = 0\n",
    "        for epoch in range(self.start_step, params['max_epochs']):\n",
    "            params['lr_scheduler'].step()\n",
    "            print(\"learning_rate:\", params['lr_scheduler'].get_lr())\n",
    "            cul_loss = 0.0\n",
    "            self.classifier.train()\n",
    "            loss_sum = 0\n",
    "            pbar = tqdm.tqdm(iterable=range(params['train_steps']), total=params['train_steps'], leave=False)\n",
    "            for i in pbar:\n",
    "                # read samples\n",
    "                sample_ids, images, labels = train_dataset[0]\n",
    "                images = torch.FloatTensor(images).to(self.init_params['device'])\n",
    "                labels = torch.FloatTensor(labels.float()).to(self.init_params['device'])\n",
    "                # Forward pass\n",
    "                outputs = self.classifier(images)\n",
    "                loss = self.loss(target=labels, input=outputs)\n",
    "                loss_sum += loss.item()\n",
    "                # Backward and optimize\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                s = 'training loss: %.4f' % (loss_sum/(i+1))\n",
    "                pbar.set_description(s)\n",
    "            if (epoch+1)% params['display'] == 0:\n",
    "                ss = \"[%04d/%04d]   \" %(epoch+1, params['max_epochs'])\n",
    "                res, s, delta = self.evaluation(train_dataset, params['train_steps'], params['pivot'], train=True)\n",
    "                \n",
    "                self.history['train_loss'].append(res['loss'])\n",
    "                self.history['train_pre'].append(res['precision'])\n",
    "                self.history['train_rec'].append(res['recall'])\n",
    "                self.history['train_f1'].append(res['f1'])\n",
    "                ss += \"train%s\\t\" % s\n",
    "                if val_dataset is not None:\n",
    "                    res, s = self.evaluation(val_dataset, params['valid_steps'], params['pivot'])\n",
    "                    self.history['valid_loss'].append(res['loss'])\n",
    "                    self.history['valid_pre'].append(res['precision'])\n",
    "                    self.history['valid_rec'].append(res['recall'])\n",
    "                    self.history['valid_f1'].append(res['f1'])\n",
    "                    ss += \"valid%s\\t\" % s\n",
    "                    if res['loss']<best_loss:\n",
    "                        self.save(model_name='best_model_loss%.4f'%res['loss'], global_step=epoch+1)\n",
    "                        best_loss = res['loss']\n",
    "                    if res['f1']>best_f1:\n",
    "                        self.save(model_name='best_model_score%.4f'%res['f1'], global_step=epoch+1)\n",
    "                        best_f1 = res['f1']\n",
    "                log.logger.info(ss)\n",
    "                # 调整loss的阈值\n",
    "                if params['loss_weight_tune'] is True:\n",
    "                    delta = delta.astype(np.float)/4.0\n",
    "                    self.loss.alpha = self.loss.alpha + \\\n",
    "                        1*self.loss.alpha*torch.Tensor(delta).float().to(self.init_params['device'])\n",
    "                    self.loss.alpha = torch.clamp(self.loss.alpha, min=0.05, max=0.95)\n",
    "                    print(self.loss.alpha)\n",
    "                #print(ss)\n",
    "        self.start_step = params['max_epochs']\n",
    "        return self.history\n",
    "    def evaluation(self, dataset, batchs=None, pivot=0, train=False):\n",
    "        \"\"\"\n",
    "            如果为训练集，由于shuffle的缘故，num_times=1\n",
    "            如果为测试集，为了保证估计准确，num_times可大于1\n",
    "        \"\"\"\n",
    "        self.classifier.to(self.init_params['device'])\n",
    "        y_pred, y_true = [], []\n",
    "        n_batchs = len(dataset) if batchs is None else batchs\n",
    "        loss = []\n",
    "        self.classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm.tqdm(iterable=range(n_batchs), total=n_batchs, leave=False)\n",
    "            for i in pbar:\n",
    "                # read samples\n",
    "                sample_ids, batch_Xs, batch_ys = dataset[0]\n",
    "                y_true.append(batch_ys.numpy().astype(np.int32))\n",
    "\n",
    "                batch_Xs = torch.FloatTensor(batch_Xs).to(self.init_params['device'])\n",
    "                batch_ys = torch.FloatTensor(batch_ys.float()).to(self.init_params['device'])\n",
    "\n",
    "                # Forward pass\n",
    "                batch_y_preds = self.classifier(batch_Xs)\n",
    "                batch_y_cost = self.loss(batch_y_preds, batch_ys).detach().cpu().numpy()\n",
    "                batch_y_preds = batch_y_preds.detach().cpu().numpy()\n",
    "\n",
    "                batch_y_preds = batch_y_preds>pivot #因为这里输出的是logit\n",
    "\n",
    "                batch_y_preds = batch_y_preds.astype(np.int32)\n",
    "                loss.append(batch_y_cost)\n",
    "                y_pred.append(batch_y_preds)\n",
    "                #print('evaluation [%03d/%03d]' % (i+1, n_batchs), end='\\r')\n",
    "            \n",
    "        y_true = np.concatenate(y_true, 0)\n",
    "        y_pred = np.concatenate(y_pred, 0)\n",
    "\n",
    "        precision = metrics.classification.precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "        recall = metrics.classification.recall_score(y_true, y_pred, average='macro')\n",
    "        f1 = metrics.classification.f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        if train is True:\n",
    "            #为focall loss的权重作准备\n",
    "            precision_arr = metrics.classification.precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "            recall_arr = metrics.classification.recall_score(y_true, y_pred, average=None)\n",
    "            delta = precision_arr+0.1 - recall_arr\n",
    "        res = {\n",
    "            'loss': np.mean(loss),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        s = \"(loss:%.4f, \"% res['loss']\n",
    "        s += \"pre:%.4f, \"% precision\n",
    "        s += \"rec:%.4f, \"% recall\n",
    "        s += \"f1:%.4f) \"% f1\n",
    "        if train is False:\n",
    "            return res, s\n",
    "        else:\n",
    "            return res, s, delta\n",
    "    def predict(self, dataset, is_test=False, batchs=None):\n",
    "        self.classifier.to(self.init_params['device'])\n",
    "        ids, y_pred, y_true = [], [], []\n",
    "        n_batchs = len(dataset) if batchs is None else batchs\n",
    "        self.classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm.tqdm(iterable=range(n_batchs), total=n_batchs, leave=False)\n",
    "            for i in pbar:\n",
    "                # read samples\n",
    "                record = dataset[0]\n",
    "                sample_ids, batch_Xs = record[0], record[1]\n",
    "                if is_test is False:\n",
    "                    batch_ys = record[2].numpy().astype(np.int32)\n",
    "                    y_true.append(batch_ys)\n",
    "\n",
    "                ids.append(sample_ids.numpy())\n",
    "                batch_Xs = torch.FloatTensor(batch_Xs).to(self.init_params['device'])\n",
    "                # Forward pass\n",
    "                batch_y_preds = self.classifier.predict(batch_Xs)\n",
    "                batch_y_preds = batch_y_preds.detach().cpu().numpy()\n",
    "                y_pred.append(batch_y_preds)\n",
    "                #print(\"predict [%04d/%04d]\" % (i+1, n_batchs), end='\\r')\n",
    "        ids = np.concatenate(ids, 0).ravel()\n",
    "        if is_test is False:\n",
    "            y_true = np.concatenate(y_true, 0)\n",
    "        y_pred = np.concatenate(y_pred, 0)\n",
    "        df ={'ids': ids, 'y_true': y_true, 'y_pred':y_pred}\n",
    "        return df\n",
    "    def save(self, save_path=None, model_name=None, global_step=1):\n",
    "        if save_path is None:\n",
    "            save_path = self.save_path+'/'+self.init_params['model_name']+'/'\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        filepath = save_path+model_name #+'_'+str(global_step)\n",
    "        torch.save(self.classifier.state_dict(), filepath)\n",
    "    def restore(self, restore_path=None, model_name=None):\n",
    "        if restore_path is None:\n",
    "            restore_path = self.save_path+'/'+self.init_params['model_name']+'/'\n",
    "        #kpt = tf.train.lastest_checkpoint(restore_path)\n",
    "        kpt = restore_path + model_name\n",
    "        self.classifier.load_state_dict(torch.load(kpt))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:53.544119Z",
     "start_time": "2018-11-28T14:00:53.537880Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def visualization(history):\n",
    "    min_ = 100000\n",
    "    for key in history.keys():\n",
    "        min_ = min(min_, len(history[key]))\n",
    "    for key in history.keys():\n",
    "        history[key] = history[key][:min_]\n",
    "    df = pd.DataFrame(history)\n",
    "    rows, cols = 1, 4\n",
    "    plt.figure(figsize=[4*cols, 4*rows])\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.plot(df[['train_loss', 'valid_loss']])\n",
    "    plt.legend(['train_loss', 'valid_loss'])\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.plot(df[['train_pre', 'valid_pre']])\n",
    "    plt.legend(['train_pre', 'valid_pre'])\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.plot(df[['train_rec', 'valid_rec']])\n",
    "    plt.legend(['train_rec', 'valid_rec'])\n",
    "\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.plot(df[['train_f1', 'valid_f1']])\n",
    "    plt.legend(['train_f1', 'valid_f1'])\n",
    "\n",
    "    plt.tight_layout() #设置默认的间距\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:00:53.893875Z",
     "start_time": "2018-11-28T14:00:53.891278Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # compute statis mean and std\n",
    "#import pandas as pd\n",
    "# def compute_mean_std(dataset, is_test=False):\n",
    "#     res = {'mean':[], 'X2':[]}\n",
    "#     for i in range(len(dataset)*1):\n",
    "#         if is_test is False:\n",
    "#             _, imgs, _ = dataset[0]\n",
    "#         else:\n",
    "#             _, imgs = dataset[0]\n",
    "#         imgs = imgs.numpy()\n",
    "#         res['mean'].append(np.mean(imgs, axis=(0, 2,3)))\n",
    "#         res['X2'].append(np.mean(imgs*imgs, axis=(0,2,3)))\n",
    "#         print('[%03d/%03d]' % (i+1, len(dataset)), end='\\r')\n",
    "#     res['mean'] = np.stack(res['mean'], axis=0)\n",
    "#     res['mean'] = np.mean(res['mean'], axis=0)\n",
    "#     res['X2'] = np.stack(res['X2'], axis=0)\n",
    "#     res['X2'] = np.mean(res['X2'], axis=0)\n",
    "#     res['std'] = np.sqrt(res['X2']-res['mean']*res['mean'])\n",
    "#     return res['mean'], res['std']\n",
    "#GRBY\n",
    "# test: mean:[0.04531973, 0.05907663, 0.04067749, 0.05924788], std: [0.1048129 , 0.12819295, 0.1317363 , 0.12461057]\n",
    "# GRBY_landmarks:\n",
    "#whole: mean: [0.06306252, 0.10562667, 0.07949321, 0.10858839], std: [0.80647908, 0.82780679, 0.82680373, 0.8309615 ]\n",
    "#train: mean: [0.06974462, 0.07274897, 0.0646682 , 0.0757419 ], std: [1.070733 , 1.027409 , 1.0482745, 1.0303733]\n",
    "# test: mean:[0.04531973, 0.19292594, 0.11885765, 0.19580479], std:[0.1048129 , 0.2978085 , 0.23873846, 0.3014688 ]\n",
    "\n",
    "# df_test = '/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/labels/baseline/test.csv'\n",
    "# df_test = pd.read_csv(df_test)\n",
    "# df_test.insert(loc=0, column='No', value=df_test.index.values)\n",
    "# test_dataset = MyLoader(df_test, batch_size=16, num_workers=4, is_test=True)\n",
    "#compute_mean_std(test_dataset, is_test=True)\n",
    "\n",
    "# df_train = '/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/labels/baseline/train.csv'\n",
    "# df_train = pd.read_csv(df_train)\n",
    "# df_train.insert(loc=0, column='No', value=df_train.index.values)\n",
    "# train_dataset = MyLoader(df_train, batch_size=16, num_workers=4)\n",
    "# compute_mean_std(train_dataset, is_test=False)\n",
    "\n",
    "# a = 31072\n",
    "# b = 11702\n",
    "# np.array([1.070733 , 1.027409 , 1.0482745, 1.0303733])*(a/(a+b))+np.array([0.1048129 , 0.2978085 , 0.23873846, 0.3014688 ])*(b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "df_train = '/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/labels/baseline/train.csv'\n",
    "df_train = pd.read_csv(df_train)\n",
    "df_train.insert(loc=0, column='No', value=df_train.index.values)\n",
    "train_index, valid_index = train_test_split(df_train.index.values, test_size=0.33, random_state=0, shuffle=True)\n",
    "train_df, valid_df = df_train.loc[train_index], df_train.loc[valid_index]\n",
    "train_df = shuffle(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:01:02.292067Z",
     "start_time": "2018-11-28T14:00:53.895492Z"
    },
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MyLoader(train_df, img_aug=True, shuffle=True, batch_size=16, num_workers=4)\n",
    "valid_dataset = MyLoader(valid_df, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:08:37.327416Z",
     "start_time": "2018-11-28T03:08:37.319337Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# del train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     2,
     8,
     64,
     112,
     143,
     150,
     156
    ]
   },
   "source": [
    "# work_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:01:13.756842Z",
     "start_time": "2018-11-28T14:01:07.912566Z"
    }
   },
   "outputs": [],
   "source": [
    "my_net = MyPredictor(num_output=28).to(device)\n",
    "my_loss = FocalLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:01:26.789676Z",
     "start_time": "2018-11-28T14:01:26.784278Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "my_optimizer = optim.Adam(params=my_net.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:01:30.938143Z",
     "start_time": "2018-11-28T14:01:30.935197Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "work_flow = WorkFlow(classifier=my_net, loss=my_loss, optim=my_optimizer, \\\n",
    "                     save_path='./result', model_name=MODEL_NAME, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Logger('./result/'+MODEL_NAME+'/all.log',level='info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:57:06.091509Z",
     "start_time": "2018-11-25T05:57:02.902Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# #work_flow.set_lr_weightdecay(lr=1e-6)\n",
    "# #res = work_flow.lr_find(train_dataset, n_points=150, n_repeat=1)\n",
    "# lr = np.array(res['lr']).reshape((-1, 1))[:,0]\n",
    "# loss = np.array(res['loss'])\n",
    "# loss = [np.mean(loss[:i+1]) if i<10 else np.mean(loss[i-5:i+1]) for i, val in enumerate(loss)]\n",
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(6*2, 4*1))\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.lineplot(x=range(len(lr)),y=lr)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('learning rate')\n",
    "# plt.subplot(1,2,2)\n",
    "# sns.lineplot(x=lr, y=loss)\n",
    "# plt.semilogx()\n",
    "# plt.xlabel('learning rate')\n",
    "# plt.ylabel('loss')\n",
    "# plt.savefig('./result/'+MODEL_NAME+'/lr_plot.jpeg')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hide_input": false
   },
   "source": [
    "## fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T07:14:09.264706Z",
     "start_time": "2018-11-28T07:14:09.261363Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# work_flow.set_lr_weightdecay(weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:32:04.987772Z",
     "start_time": "2018-11-28T03:32:04.979424Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#work_flow.set_lr_weightdecay(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:10:45.883059Z",
     "start_time": "2018-11-28T14:10:45.879924Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# alpha = [0.5863, 0.7342, 0.6891, 0.7998, 0.6907, 0.7061, 0.6084, 0.6644, 0.7492,\n",
    "#         0.6276, 0.5353, 0.7328, 0.7579, 0.5598, 0.7412, 0.7759, 0.7217, 0.6049,\n",
    "#         0.7273, 0.6870, 0.7145, 0.6574, 0.7434, 0.7026, 0.7813, 0.6012, 0.6776,\n",
    "#         0.9500]\n",
    "# work_flow.loss = MyFocalLossWithWeight(alpha=alpha, device=device)\n",
    "#work_flow.loss = FocalLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:02:49.368840Z",
     "start_time": "2018-11-28T14:02:48.617447Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#work_flow.restore(model_name='best_model_score0.3439')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:32:53.236720Z",
     "start_time": "2018-11-28T10:32:53.231288Z"
    }
   },
   "outputs": [],
   "source": [
    "# tip1: 训练时 平均池化，测试时最大池化; 平均池化可以使网络在训练过程中充分学习，最大池化使得检测的更突出\n",
    "# tip2: 同样的f1，loss越大越好。loss越大，说明此时没有陷入局部极小值\n",
    "#work_flow.start_step = lr_sched.last_epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T14:02:30.593801Z",
     "start_time": "2018-11-28T14:02:30.544547Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def delete_files(path_dir = './result/'+MODEL_NAME):\n",
    "    files = sorted(os.listdir(path_dir))\n",
    "    loss_file = [val for val in files if re.match(pattern=r'[\\.\\_/a-zA-Z0-9]*loss[0-9\\.]*', string=val) is not None]\n",
    "    score_file = [val for val in files if re.match(pattern=r'[\\.\\_/a-zA-Z0-9]*score[0-9\\.]*', string=val) is not None]\n",
    "    loss_file = sorted(loss_file)[4:]\n",
    "    score_file = sorted(score_file)[:-4]\n",
    "    for file in loss_file:\n",
    "        tmp = os.path.join(path_dir, file)\n",
    "        os.remove(tmp)\n",
    "    for file in score_file:\n",
    "        tmp = os.path.join(path_dir, file)\n",
    "        os.remove(tmp)\n",
    "delete_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "lr_sched = lr_scheduler.StepLR(optimizer=work_flow.optim, gamma=0.1, step_size=10, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-28T14:10:56.220Z"
    },
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_params = {\n",
    "    'max_epochs': 50,\n",
    "    'train_steps': int(len(train_dataset)),\n",
    "    'valid_steps': int(len(valid_dataset)),\n",
    "    'display': 1,\n",
    "    'pivot': 0,\n",
    "    'loss_weight_tune': False,\n",
    "    'lr_scheduler':lr_sched,\n",
    "}\n",
    "work_flow.train(train_dataset, train_params, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:58:09.888428Z",
     "start_time": "2018-11-25T10:58:09.284620Z"
    }
   },
   "outputs": [],
   "source": [
    "visualization(work_flow.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:57:06.102279Z",
     "start_time": "2018-11-25T05:57:03.397Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tmp = train_df.drop(labels=['Id'], axis=1).sum(axis=0)/train_df.shape[0]\n",
    "sns.barplot(x=tmp.index, y=tmp.values, order=tmp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:57:06.103683Z",
     "start_time": "2018-11-25T05:57:03.403Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tmp = valid_df.drop(labels=['Id'], axis=1).sum(axis=0)/valid_df.shape[0]\n",
    "g = sns.barplot(x=tmp.index, y=tmp.values, order=tmp.index)\n",
    "for x, y in enumerate(tmp.values):\n",
    "    g.text(x, y, round(y,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search optimal threshold\n",
    "    利用leastsq在验证集为每个标签寻找最佳阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:30:50.754930Z",
     "start_time": "2018-11-28T13:30:50.743733Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opm  #命名为opm， 为了避免与torch.optim 命名重复\n",
    "from sklearn.metrics import classification\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0+np.exp(-x))\n",
    "def compute_softF1(target, pred, threshold=0.5, scale=50):\n",
    "    pred = sigmoid(scale*(pred-threshold))\n",
    "    target = target.astype(np.float)\n",
    "    score = 2 * (pred * target).sum(axis=0) / ((pred + target).sum(axis=0) + 1e-6)\n",
    "    return score\n",
    "def find_optimal_threshold(target, pred):\n",
    "    num_labels = target.shape[1]\n",
    "    w0 = 0.5 * np.ones(num_labels) #初始阈值\n",
    "    alpha = 1e-5  #偏离正常阈值的惩罚系数\n",
    "    residual_error = lambda w: np.concatenate([compute_softF1(target, pred, w)-1, alpha*(w-0.5)], axis=0)\n",
    "    w, success = opm.leastsq(residual_error, w0)\n",
    "    return w\n",
    "def test_func(targ=None, pred=None):\n",
    "    if targ is None or pred is None:\n",
    "        targ = np.random.randint(low=0, high=2, size=(10, 10))\n",
    "        pred = np.random.rand(10,10)\n",
    "    #0.5作为预测值\n",
    "    print(\"oridinary threshold\".center(40, '#'))\n",
    "    y = (pred>0.5).astype(np.int32)\n",
    "    tmp = classification.f1_score(y_true=targ, y_pred=y, average='macro')\n",
    "    print(\"macro f1: %.4f\" % tmp)\n",
    "    \n",
    "    # 寻找最佳阈值\n",
    "    print(\"optimal threshold\".center(40, '#'))\n",
    "    w = find_optimal_threshold(targ, pred)\n",
    "    print(\"optimal threshods:\", w)\n",
    "    y = (pred>w).astype(np.int32)\n",
    "    tmp = classification.f1_score(y_true=targ, y_pred=y, average='macro')\n",
    "    print(\"macro f1: %.4f\" % tmp)\n",
    "    \n",
    "    print(\"trunc optimal threshold\".center(40, '#'))\n",
    "    w_trunc = w.copy()\n",
    "    w_trunc[w_trunc<0.1] = 0.1 #设置最低阈值\n",
    "    w_trunc[w_trunc>0.9] = 0.9 #设置最大阈值\n",
    "    print(\"trunc optimal threshods:\", w_trunc)\n",
    "    y = (pred>w_trunc).astype(np.int32)\n",
    "    tmp = classification.f1_score(y_true=targ, y_pred=y, average='macro')\n",
    "    print(\"trunc macro f1: %.4f\" % tmp)\n",
    "    return w, w_trunc\n",
    "#test_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:35:41.591232Z",
     "start_time": "2018-11-28T13:33:41.667308Z"
    }
   },
   "outputs": [],
   "source": [
    "# 通过训练集计算最佳阈值\n",
    "#work_flow.restore(model_name='best_model_score0.6743')\n",
    "res_train = work_flow.predict(valid_dataset, is_test=False)\n",
    "res_train['y_prob'] = sigmoid(res_train['y_pred'])\n",
    "w, w_trunc = test_func(res_train['y_true'], sigmoid(res_train['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:50:49.913539Z",
     "start_time": "2018-11-28T13:50:49.868922Z"
    }
   },
   "outputs": [],
   "source": [
    "p = classification.precision_score(y_true=res_train['y_true'], y_pred=res_train['y_prob']>0.4, average='macro')\n",
    "r = classification.recall_score(y_true=res_train['y_true'], y_pred=res_train['y_prob']>0.4, average='macro')\n",
    "f1 = classification.f1_score(y_true=res_train['y_true'], y_pred=res_train['y_prob']>w_trunc, average='macro')\n",
    "print(p)\n",
    "print(r)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:57:06.107886Z",
     "start_time": "2018-11-25T05:57:03.593Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tmp = pd.DataFrame(data=(res_train['y_prob']>w_trunc).astype(np.int32))\n",
    "# d = tmp.sum(axis=1)\n",
    "# index = d[d==0].index\n",
    "# for i in index:\n",
    "#     k = np.argmax(res_train['y_prob'][i])\n",
    "#     print(k, end=' ')\n",
    "#     tmp.iloc[i,k] = 1\n",
    "#classification.f1_score(y_true=res_train['y_true'], y_pred=tmp.values, average='macro')\n",
    "tmp = tmp.sum(axis=0)/valid_df.shape[0]\n",
    "g = sns.barplot(x=tmp.index, y=tmp.values, order=tmp.index)\n",
    "for x, y in enumerate(tmp.values):\n",
    "    g.text(x, y, round(y,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:57:06.109226Z",
     "start_time": "2018-11-25T05:57:03.601Z"
    }
   },
   "outputs": [],
   "source": [
    "# 观察最佳阈值在验证集上的表现\n",
    "res_valid = work_flow.predict(valid_dataset, is_test=False)\n",
    "res_valid['y_prob'] = sigmoid(res_valid['y_pred'])\n",
    "res_valid['y_pred'] = (res_valid['y_prob']>0.5).astype(np.int32)\n",
    "macro_f1 = classification.f1_score(y_true=res_valid['y_true'], y_pred=res_valid['y_pred'], average='macro')\n",
    "print(\"ordianry threshold macro f1 on valid dataset: %.4f\" % macro_f1)\n",
    "res_valid['y_pred'] = (res_valid['y_prob']>w_trunc).astype(np.int32)\n",
    "macro_f1 = classification.f1_score(y_true=res_valid['y_true'], y_pred=res_valid['y_pred'], average='macro')\n",
    "print(\"trunc threshold macro f1 on valid dataset: %.4f\" % macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:46:08.164716Z",
     "start_time": "2018-11-28T13:42:25.697383Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/labels/baseline/testA.csv')\n",
    "test_data_kw = {\n",
    "    'img_size': IMAGE_SIZE,\n",
    "    'num_labels':28, \n",
    "    'num_epochs':1, #训练集验证集为None,测试集为1\n",
    "    'img_dir':'/sdadata/zzq/dataset/HumanProteinAtlasImageClassification/images',\n",
    "    \n",
    "    'batch_size': 16,\n",
    "    'shuffle': False, #训练集shuffle为True,否则为False\n",
    "    'num_threads': 2,  #生成测试结果时用单线程，以保证样本不乱序（多线程由于异步运算会打乱样本的顺序）\n",
    "    'compute_statis': False,\n",
    "    'img_aug': False,\n",
    "    'is_test': True, #区分从哪个目录下导入文件, train or testA\n",
    "}\n",
    "#work_flow.restore(model_name='best_model_score0.5249')\n",
    "test_dataset = MyDataset(df_test, **test_data_kw)\n",
    "res = work_flow.predict(test_dataset, is_test=True)\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:55:52.779318Z",
     "start_time": "2018-11-28T13:55:52.767533Z"
    }
   },
   "outputs": [],
   "source": [
    "index = np.argsort(res['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:31:50.635044Z",
     "start_time": "2018-11-28T13:31:50.629412Z"
    }
   },
   "outputs": [],
   "source": [
    "del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyse predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:58:55.017171Z",
     "start_time": "2018-11-28T13:58:54.923904Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_ids = [val.decode() for val in sorted(res['ids'])]\n",
    "res['y_prob'] = sigmoid(res['y_pred'])\n",
    "preds = np.array(res['y_prob'][index]>w_trunc-0.02, dtype=np.int32) # w_trunc由 step4:search optimal threshold 得到\n",
    "df = pd.DataFrame(preds)\n",
    "df['Id'] = sample_ids\n",
    "df.set_index(keys=['Id'], drop=True, append=False, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:58:58.429722Z",
     "start_time": "2018-11-28T13:58:57.929566Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "label_count = df.sum(axis=0)/df.shape[0]\n",
    "g = sns.barplot(x=label_count.index, y=label_count.values, order=label_count.index)\n",
    "for x, y in enumerate(label_count.values):\n",
    "    g.text(x, y, round(y,4))\n",
    "plt.title('label frequence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:59:01.424386Z",
     "start_time": "2018-11-28T13:59:01.278717Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_labels_count = df.sum(axis=1)\n",
    "LCard = sample_labels_count.mean()\n",
    "sample_labels_count = sample_labels_count.value_counts()/df.shape[0]\n",
    "sample_labels_count = sample_labels_count.sort_index()\n",
    "g = sns.barplot(x=sample_labels_count.index, y=sample_labels_count.values, order=sample_labels_count.index)\n",
    "for x, y in enumerate(sample_labels_count.values):\n",
    "    g.text(x-0.25, y, round(y,4))\n",
    "g.title = g.text(1.5, 0.55, 'Label Card: %.2f' % LCard)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T13:59:08.586588Z",
     "start_time": "2018-11-28T13:59:06.594756Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Predicted'] = ''\n",
    "\n",
    "#     temp = np.where(out>= 0)\n",
    "#     id_list = [ str(x) for x in temp[0] ]\n",
    "#     pred = ' '.join(id_list)\n",
    "#     pred_test.append(pred)\n",
    "\n",
    "def func(row):\n",
    "    for i, val in enumerate(row.values[:-1]):\n",
    "        if val == 1:\n",
    "            row['Predicted'] += (' '+str(i))\n",
    "    row['Predicted'] = row['Predicted'][1:]  #去掉前面的空格\n",
    "    return row\n",
    "df = df.apply(func, axis=1)\n",
    "df_res = df[['Predicted']]\n",
    "df_res.to_csv('./result/upload/'+MODEL_NAME +'/score_0.54.csv', index=True, encoding='utf-8')\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "764px",
    "left": "23px",
    "top": "133px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
